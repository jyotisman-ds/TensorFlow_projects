{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Classification of Yelp Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading libraries and datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "executionInfo": {
     "elapsed": 74,
     "status": "error",
     "timestamp": 1611516267887,
     "user": {
      "displayName": "Jyotisman Sahoo",
      "photoUrl": "",
      "userId": "12108646937534858560"
     },
     "user_tz": 360
    },
    "id": "ApDSynvOEUgj",
    "outputId": "d4137fff-3549-431e-b432-f170a7e9353a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LKtJ7PO-G8VU"
   },
   "outputs": [],
   "source": [
    "# Obtaining the relevant data from tensorflow_datasets\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# split the provided test set into a validation and test set \n",
    "train_ds, info = tfds.load('yelp_polarity_reviews', split = 'train', as_supervised = True, with_info=True)\n",
    "valid_ds = tfds.load('yelp_polarity_reviews', split = 'test[:50%]', as_supervised = True)\n",
    "test_ds = tfds.load('yelp_polarity_reviews', split = 'test[50%:]', as_supervised = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 241,
     "status": "ok",
     "timestamp": 1611464784448,
     "user": {
      "displayName": "Jyotisman Sahoo",
      "photoUrl": "",
      "userId": "12108646937534858560"
     },
     "user_tz": 360
    },
    "id": "4bB3jNJXIz90",
    "outputId": "ca14dd32-356d-4ff4-f8f9-272176d54711"
   },
   "outputs": [],
   "source": [
    "# Let's check out a few examples and their labels\n",
    "for text, label in train_ds.take(2).as_numpy_iterator():\n",
    "  print(\"Text - {}\".format(text))\n",
    "  print(\"Label - {}\".format(label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing some common stop words from the given datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JsevyZTzmd6Y"
   },
   "outputs": [],
   "source": [
    "stopwords = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kpkNKJE3mZHp"
   },
   "outputs": [],
   "source": [
    "# Function to eliminate stopwords \n",
    "def remove_stopwords(text, label):\n",
    "    text = \" \" + text        # needed for to capture some stopwords appearing at the beginnning of the string \n",
    "    text = tf.strings.lower(text)  \n",
    "    for word in stopwords:\n",
    "        token = \" \" + word + \" \"\n",
    "        text = tf.strings.regex_replace(text, token, \" \")\n",
    "    return text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zu8GMY9MnkcW"
   },
   "outputs": [],
   "source": [
    "train_ds = train_ds.map(remove_stopwords)\n",
    "valid_ds = valid_ds.map(remove_stopwords)\n",
    "test_ds = test_ds.map(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check out a few examples with the stopwords removed\n",
    "for text, label in train_ds.take(2).as_numpy_iterator():\n",
    "  print(\"Text - {}\".format(text))\n",
    "  print(\"Label - {}\".format(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yWd4HOBGrFTJ"
   },
   "outputs": [],
   "source": [
    "# saving just the text of the train_data in a variable for tokenizing\n",
    "train_ds_text = train_ds.map(lambda text, label : text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing the train data using the TextVectorization method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AgdZcrxWYiRF"
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 120 # truncating the sentences to this length. Padding is taken care of automatically.\n",
    "VOCAB_SIZE = 10000 # Maximum vocabulary list to consider\n",
    "\n",
    "tokenize_layer = TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    output_mode='int', # indexing tokens with a series of integers\n",
    "    output_sequence_length=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "23q0qJmEY1zD"
   },
   "outputs": [],
   "source": [
    "# Tokenization using the train data only (DONT USE VALID/TEST DATA - DONT WANT DATA LEAKAGE)\n",
    "tokenize_layer.adapt(train_ds_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the tokenize_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eTxODxRsbo9N"
   },
   "outputs": [],
   "source": [
    "# function to convert the texts into token indices \n",
    "def tokenize_text(text, label):\n",
    "  text = tf.expand_dims(text, -1)\n",
    "  return tokenize_layer(text)[0], label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 260,
     "status": "ok",
     "timestamp": 1611465199057,
     "user": {
      "displayName": "Jyotisman Sahoo",
      "photoUrl": "",
      "userId": "12108646937534858560"
     },
     "user_tz": 360
    },
    "id": "TCfIIuZ8Xioj",
    "outputId": "1ceed44b-0081-4057-e4e3-44b785ea69a2"
   },
   "outputs": [],
   "source": [
    "# Consider a single example ----->\n",
    "example1, label1  = next(iter(train_ds))\n",
    "print(\"Example_text--> \", example1)\n",
    "print(\"Label--> \", label1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 249,
     "status": "ok",
     "timestamp": 1611465203291,
     "user": {
      "displayName": "Jyotisman Sahoo",
      "photoUrl": "",
      "userId": "12108646937534858560"
     },
     "user_tz": 360
    },
    "id": "9uf0bFWIcjL8",
    "outputId": "fdb4181f-cc7b-4224-a0d5-61bdd0b1737f"
   },
   "outputs": [],
   "source": [
    "# Now let's view its tokenized form\n",
    "print(\"vectorized example text:\",\n",
    "      tokenize_text(example1, label1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 248,
     "status": "ok",
     "timestamp": 1611465212836,
     "user": {
      "displayName": "Jyotisman Sahoo",
      "photoUrl": "",
      "userId": "12108646937534858560"
     },
     "user_tz": 360
    },
    "id": "xo6EH6VTc2Em",
    "outputId": "f1465a90-89fc-4d20-d742-e587424ea695"
   },
   "outputs": [],
   "source": [
    "# Checking a few of the indices to see if the mapping is as expected --->\n",
    "\n",
    "print(\"1 --> {}\".format(tokenize_layer.get_vocabulary()[1]))\n",
    "print(\"2 --> {}\".format(tokenize_layer.get_vocabulary()[3840]))\n",
    "print(\"Vocabulary size --> {}\".format(len(tokenize_layer.get_vocabulary())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can in principle convert all the text data into vectors as shown in the above example before feeding it into the model. This can speed up the training process but then one has to save the tokenizing indices separately for inference later as this is unique to the trained model. To generalize, we include the TextVectorization layer as part of the model building process. For anyone interested, the code to convert the data into indices before feeding to the model is also provided in the cell below but is commented out here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gH0IDUlleGC4"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "# Converting texts into token indices learned from the training files\n",
    "\n",
    "int_train_ds = train_ds.map(tokenize_text)\n",
    "int_valid_ds = valid_ds.map(tokenize_text)\n",
    "int_test_ds = test_ds.map(tokenize_text)\n",
    "\n",
    "\n",
    "for text_vec, _ in int_train_ds.take(2).as_numpy_iterator():\n",
    "  print(\"Text - {}\".format(text_vec))\n",
    "  \n",
    "\n",
    "# cache the prepared dataset in memory\n",
    "int_train_ds = int_train_ds.cache()\n",
    "int_valid_ds = int_valid_ds.cache()\n",
    "int_test_ds = int_test_ds.cache()\n",
    "\n",
    "# Preparing the dataset to be loaded into the model \n",
    "BUFFER_SIZE = 300000\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# for model without vectorization layer \n",
    "train_dataset = int_train_ds.shuffle(BUFFER_SIZE)\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "validation_dataset = int_valid_ds.batch(4).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = int_test_ds.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential([tf.keras.layers.Embedding(input_dim = VOCAB_SIZE, \n",
    "                                                       output_dim = 64, input_length = MAX_SEQUENCE_LENGTH),\n",
    "                             tf.keras.layers.Conv1D(128,5,activation='relu'),\n",
    "                             tf.keras.layers.GlobalAveragePooling1D(),\n",
    "                             tf.keras.layers.Dense(32, activation='relu'),\n",
    "                             tf.keras.layers.Dense(1,activation='sigmoid')])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cache the prepared dataset in memory\n",
    "train_ds = train_ds.cache()\n",
    "valid_ds = valid_ds.cache()\n",
    "test_ds = test_ds.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the dataset to be loaded into the model \n",
    "BUFFER_SIZE = 300000\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# for model with vectorization layer\n",
    "train_dataset = train_ds.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "validation_dataset = valid_ds.batch(4).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_ds.batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([tf.keras.Input(shape = (1,), dtype = tf.string),\n",
    "                             tokenize_layer,\n",
    "                             tf.keras.layers.Embedding(input_dim = VOCAB_SIZE, \n",
    "                                                       output_dim = 64, input_length = MAX_SEQUENCE_LENGTH),\n",
    "                             tf.keras.layers.Conv1D(128,5,activation='relu'),\n",
    "                             tf.keras.layers.GlobalAveragePooling1D(),\n",
    "                             tf.keras.layers.Dense(32, activation='relu'),\n",
    "                             tf.keras.layers.Dense(1,activation='sigmoid')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3gnW9afhIxuI"
   },
   "outputs": [],
   "source": [
    "model.compile(loss = 'binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 452095,
     "status": "ok",
     "timestamp": 1611466172299,
     "user": {
      "displayName": "Jyotisman Sahoo",
      "photoUrl": "",
      "userId": "12108646937534858560"
     },
     "user_tz": 360
    },
    "id": "KT7dIu_bI03r",
    "outputId": "9f02a1f8-46f8-4ff0-e7bb-237993a5c8bf"
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 5\n",
    "history = model.fit(train_dataset, epochs=NUM_EPOCHS, validation_data=validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_path = \"./my_models/{}\".format('sentiment_classifier_withVectorizer')\n",
    "\n",
    "model.save(saved_model_path, save_format = 'tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = tf.keras.models.load_model(\"./my_models/{}\".format('sentiment_classifier_withVectorizer'))\n",
    "\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 541
    },
    "executionInfo": {
     "elapsed": 615,
     "status": "ok",
     "timestamp": 1611466172934,
     "user": {
      "displayName": "Jyotisman Sahoo",
      "photoUrl": "",
      "userId": "12108646937534858560"
     },
     "user_tz": 360
    },
    "id": "eOUhOMUbIbYB",
    "outputId": "ae480d84-0e22-4cc5-fcd8-fcdfd1000224"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_graphs(history, string):\n",
    "  plt.plot(history.history[string])\n",
    "  plt.plot(history.history['val_'+string])\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(string)\n",
    "  plt.legend([string, 'val_'+string])\n",
    "  plt.show()\n",
    "\n",
    "plot_graphs(history, 'accuracy')\n",
    "plot_graphs(history, 'loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c2K56RnWTpJy"
   },
   "source": [
    "Even though the accuracies are high enough there is subastantial overfitting only after the first 2 epochs if the stopwords are not removed (one can check this by keeping the stopwords in the splits). After removing the stopwords (which is what we do here eventually), we still get overfitting but atleast towards the end of the training now. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2CLP1O0sIlTi"
   },
   "source": [
    "When we need to deploy the model, it is better to include the TextVectorization layer into the model itself since it contains the Tokenizer and the same indexing needs to be used for any unseen data. This will significantly increase the training time but probably worth it. Let's take a look at it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SkhGCM2DLPfY"
   },
   "source": [
    "## TextVectorization with a custom standardize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3dUT_dovTbuw"
   },
   "outputs": [],
   "source": [
    "@tf.keras.utils.register_keras_serializable()\n",
    "def operations(text):\n",
    "  text = \" \" + text\n",
    "  text = tf.strings.lower(text)\n",
    "  stopwords = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]\n",
    "  for word in stopwords:\n",
    "    token = \" \" + word + \" \"\n",
    "    text = tf.strings.regex_replace(text, token, \" \")\n",
    "  remove_regex = f'[{re.escape(string.punctuation)}]'\n",
    "  text = tf.strings.regex_replace(text, remove_regex, '')\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1wP8-GI1KRhP"
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 120\n",
    "VOCAB_SIZE = 10000\n",
    "\n",
    "tokenizer_layer = TextVectorization(\n",
    "  standardize=operations,\n",
    "  max_tokens=VOCAB_SIZE,\n",
    "  output_mode='int',\n",
    "  output_sequence_length=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "APyGlLkPLY0I"
   },
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "train_ds, info = tfds.load('yelp_polarity_reviews', split = 'train', as_supervised = True, with_info=True)\n",
    "valid_ds = tfds.load('yelp_polarity_reviews', split = 'test[:50%]', as_supervised = True)\n",
    "test_ds = tfds.load('yelp_polarity_reviews', split = 'test[50%:]', as_supervised = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wSIlIuCYMXrA"
   },
   "outputs": [],
   "source": [
    "train_only_text = train_ds.map(lambda text, label : text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TFLNXMFBLgrG"
   },
   "outputs": [],
   "source": [
    "tokenizer_layer.adapt(train_only_text.batch(2048))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 230,
     "status": "ok",
     "timestamp": 1611476836526,
     "user": {
      "displayName": "Jyotisman Sahoo",
      "photoUrl": "",
      "userId": "12108646937534858560"
     },
     "user_tz": 360
    },
    "id": "1FyU0bQoMn5d",
    "outputId": "a39d17aa-106f-4c3b-aa45-5977c1bbfc00"
   },
   "outputs": [],
   "source": [
    "example1, label1  = next(iter(train_ds))\n",
    "print(\"Example_text--> \", example1)\n",
    "print(\"Label--> \", label1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QLtjFjkCODHg"
   },
   "outputs": [],
   "source": [
    "def tokenize_text2(text, label):\n",
    "  text = tf.expand_dims(text, -1)\n",
    "  return tokenizer_layer(text)[0], label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 295,
     "status": "ok",
     "timestamp": 1611473833122,
     "user": {
      "displayName": "Jyotisman Sahoo",
      "photoUrl": "",
      "userId": "12108646937534858560"
     },
     "user_tz": 360
    },
    "id": "LZCln7YZOur0",
    "outputId": "0e30a6d6-bbbf-4c3b-9c1f-de252014a975"
   },
   "outputs": [],
   "source": [
    "print(tokenize_text2(example1,label1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 235,
     "status": "ok",
     "timestamp": 1611480245208,
     "user": {
      "displayName": "Jyotisman Sahoo",
      "photoUrl": "",
      "userId": "12108646937534858560"
     },
     "user_tz": 360
    },
    "id": "6z7U_Pi8ecis",
    "outputId": "6cb39e2f-b250-4c8c-9157-925791f27205"
   },
   "outputs": [],
   "source": [
    "tokenizer_layer.get_vocabulary()[4951]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YA_bDe9xO6CP"
   },
   "outputs": [],
   "source": [
    "# Preparing the dataset to be loaded into the model \n",
    "BUFFER_SIZE = 300000\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataset = train_ds.shuffle(BUFFER_SIZE)\n",
    "train_dataset = train_ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "validation_dataset = valid_ds.batch(4).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_ds.batch(4).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pF_E9yMVR-lp"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([tf.keras.Input(shape = (1,), dtype = tf.string),\n",
    "                             tokenizer_layer,\n",
    "                             tf.keras.layers.Embedding(input_dim = VOCAB_SIZE, \n",
    "                                                       output_dim = 64, input_length = MAX_SEQUENCE_LENGTH),\n",
    "                             tf.keras.layers.Conv1D(128,5,activation='relu'),\n",
    "                             tf.keras.layers.GlobalAveragePooling1D(),\n",
    "                             tf.keras.layers.Dense(32, activation='relu'),\n",
    "                             tf.keras.layers.Dense(1,activation='sigmoid')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C1tyy2vqU4O-"
   },
   "outputs": [],
   "source": [
    "model.compile(loss = 'binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1019818,
     "status": "ok",
     "timestamp": 1611481299508,
     "user": {
      "displayName": "Jyotisman Sahoo",
      "photoUrl": "",
      "userId": "12108646937534858560"
     },
     "user_tz": 360
    },
    "id": "0aku1KLKU9cW",
    "outputId": "fb97f273-6cb9-4113-a3f3-907435d023dd"
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 5\n",
    "history = model.fit(train_dataset, epochs=NUM_EPOCHS, validation_data=validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6333,
     "status": "ok",
     "timestamp": 1611481305850,
     "user": {
      "displayName": "Jyotisman Sahoo",
      "photoUrl": "",
      "userId": "12108646937534858560"
     },
     "user_tz": 360
    },
    "id": "YdEOqUeWVBi6",
    "outputId": "3cb08a41-56c2-4210-d739-304aeb826f56"
   },
   "outputs": [],
   "source": [
    "model.evaluate(test_dataset)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyM71Y1EhLcr+PUxL3SdhGsI",
   "collapsed_sections": [],
   "name": "sentiment_classifier.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
