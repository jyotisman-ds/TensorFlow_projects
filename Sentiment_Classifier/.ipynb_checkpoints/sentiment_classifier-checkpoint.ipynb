{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Classification of Yelp Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading libraries and datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "executionInfo": {
     "elapsed": 74,
     "status": "error",
     "timestamp": 1611516267887,
     "user": {
      "displayName": "Jyotisman Sahoo",
      "photoUrl": "",
      "userId": "12108646937534858560"
     },
     "user_tz": 360
    },
    "id": "ApDSynvOEUgj",
    "outputId": "d4137fff-3549-431e-b432-f170a7e9353a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "LKtJ7PO-G8VU"
   },
   "outputs": [],
   "source": [
    "# Obtaining the relevant data from tensorflow_datasets\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# split the provided test set into a validation and test set \n",
    "train_ds, info = tfds.load('yelp_polarity_reviews', split = 'train', as_supervised = True, with_info=True)\n",
    "valid_ds = tfds.load('yelp_polarity_reviews', split = 'test[:50%]', as_supervised = True)\n",
    "test_ds = tfds.load('yelp_polarity_reviews', split = 'test[50%:]', as_supervised = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 241,
     "status": "ok",
     "timestamp": 1611464784448,
     "user": {
      "displayName": "Jyotisman Sahoo",
      "photoUrl": "",
      "userId": "12108646937534858560"
     },
     "user_tz": 360
    },
    "id": "4bB3jNJXIz90",
    "outputId": "ca14dd32-356d-4ff4-f8f9-272176d54711"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text - b\"The Groovy P. and I ventured to his old stomping grounds for lunch today.  The '5 and Diner' on 16th St and Colter left me with little to ask for.  Before coming here I had a preconceived notion that 5 & Diners were dirty and nasty. Not the case at all.\\\\n\\\\nWe walk in and let the waitress know we want to sit outside (since it's so nice and they had misters).  We get two different servers bringing us stuff (talk about service) and I ask the one waitress for recommendations.  I didn't listen to her, of course, and ordered the Southwestern Burger w/ coleslaw and started with a nice stack of rings.\\\\n\\\\nThe Onion Rings were perfectly cooked.  They looked like they were prepackaged, but they were very crispy and I could actually bite through the onion without pulling the entire thing out (don't you hate that?!!!)\\\\n\\\\nThe Southwestern Burger was order Medium Rare and was cooked accordingly.  Soft, juicy, and pink with a nice crispy browned outer layer that can only be achieved on a well used grill.  The creaminess of the chipotle mayo paired beautifully with the green chiles.  Unfortunately, because I ate too many onion rings, I couldn't finish my burger.  What a shame!\\\\n\\\\nThe Coleslaw was just how I like it.  It's hard to find a really good coleslaw.  I prefer mine to be slightly sweet, not sour.  Too much vinegar in slaw ruins it in my opinion.  This slaw had the perfect marriage of mayo, vinegar, and sugar. Not to mention carrots...\\\\n\\\\nMy experience here was great!  The servers were top notch and kept my water full the entire time and actually chatted with us for a few minutes.\\\\n\\\\nThere is an artist guy named Ross who has been there every day for 5393 days straight. No, not an employee.  He goes there and does his art! He hasn't missed a SINGLE day!!! That's like... 15 years! So if you wanna seem to be 'in the know' ask where Ross is... They'll be able to tell you.\\\\n\\\\nTime for a nap!\"\n",
      "Label - 1\n",
      "Text - b\"Mediocre burgers - if you are in the area and want a fast food burger, Fatburger is  a better bet than Wendy's. But it is nothing to go out of your way for.\"\n",
      "Label - 0\n"
     ]
    }
   ],
   "source": [
    "# Let's check out a few examples and their labels\n",
    "for text, label in train_ds.take(2).as_numpy_iterator():\n",
    "  print(\"Text - {}\".format(text))\n",
    "  print(\"Label - {}\".format(label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing some common stop words from the given datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "JsevyZTzmd6Y"
   },
   "outputs": [],
   "source": [
    "stopwords = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "kpkNKJE3mZHp"
   },
   "outputs": [],
   "source": [
    "# Function to eliminate stopwords \n",
    "def remove_stopwords(text, label):\n",
    "    text = \" \" + text        # needed for to capture some stopwords appearing at the beginnning of the string \n",
    "    text = tf.strings.lower(text)  \n",
    "    for word in stopwords:\n",
    "        token = \" \" + word + \" \"\n",
    "        text = tf.strings.regex_replace(text, token, \" \")\n",
    "    return text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "zu8GMY9MnkcW"
   },
   "outputs": [],
   "source": [
    "train_ds = train_ds.map(remove_stopwords)\n",
    "valid_ds = valid_ds.map(remove_stopwords)\n",
    "test_ds = test_ds.map(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text - b\" groovy p. ventured old stomping grounds lunch today.  '5 diner' 16th st colter left little ask for.  coming preconceived notion 5 & diners dirty nasty. not case all.\\\\n\\\\nwe walk let waitress know want sit outside (since nice misters).  get two different servers bringing us stuff (talk service) ask one waitress recommendations.  didn't listen her, course, ordered southwestern burger w/ coleslaw started nice stack rings.\\\\n\\\\nthe onion rings perfectly cooked.  looked like prepackaged, crispy actually bite onion without pulling entire thing (don't hate that?!!!)\\\\n\\\\nthe southwestern burger order medium rare cooked accordingly.  soft, juicy, pink nice crispy browned outer layer can achieved well used grill.  creaminess chipotle mayo paired beautifully green chiles.  unfortunately, ate many onion rings, couldn't finish burger.  shame!\\\\n\\\\nthe coleslaw just like it.  hard find really good coleslaw.  prefer mine slightly sweet, not sour.  much vinegar slaw ruins opinion.  slaw perfect marriage mayo, vinegar, sugar. not mention carrots...\\\\n\\\\nmy experience great!  servers top notch kept water full entire time actually chatted us minutes.\\\\n\\\\nthere artist guy named ross every day 5393 days straight. no, not employee.  goes art! hasn't missed single day!!! like... 15 years! wanna seem 'in know' ask ross is... able tell you.\\\\n\\\\ntime nap!\"\n",
      "Label - 1\n",
      "Text - b\" mediocre burgers - area want fast food burger, fatburger  better bet wendy's. nothing go way for.\"\n",
      "Label - 0\n"
     ]
    }
   ],
   "source": [
    "# Let's check out a few examples with the stopwords removed\n",
    "for text, label in train_ds.take(2).as_numpy_iterator():\n",
    "  print(\"Text - {}\".format(text))\n",
    "  print(\"Label - {}\".format(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "yWd4HOBGrFTJ"
   },
   "outputs": [],
   "source": [
    "# saving just the text of the train_data in a variable for tokenizing\n",
    "train_ds_text = train_ds.map(lambda text, label : text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing the train data using the TextVectorization method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "AgdZcrxWYiRF"
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 120 # truncating the sentences to this length. Padding is taken care of automatically.\n",
    "VOCAB_SIZE = 10000 # Maximum vocabulary list to consider\n",
    "\n",
    "tokenize_layer = TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    output_mode='int', # indexing tokens with a series of integers\n",
    "    output_sequence_length=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "23q0qJmEY1zD"
   },
   "outputs": [],
   "source": [
    "# Tokenization using the train data only (DONT USE VALID/TEST DATA - DONT WANT DATA LEAKAGE)\n",
    "tokenize_layer.adapt(train_ds_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the tokenize_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "eTxODxRsbo9N"
   },
   "outputs": [],
   "source": [
    "# function to convert the texts into token indices \n",
    "def tokenize_text(text, label):\n",
    "  text = tf.expand_dims(text, -1)\n",
    "  return tokenize_layer(text)[0], label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 260,
     "status": "ok",
     "timestamp": 1611465199057,
     "user": {
      "displayName": "Jyotisman Sahoo",
      "photoUrl": "",
      "userId": "12108646937534858560"
     },
     "user_tz": 360
    },
    "id": "TCfIIuZ8Xioj",
    "outputId": "1ceed44b-0081-4057-e4e3-44b785ea69a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example_text-->  tf.Tensor(b\" groovy p. ventured old stomping grounds lunch today.  '5 diner' 16th st colter left little ask for.  coming preconceived notion 5 & diners dirty nasty. not case all.\\\\n\\\\nwe walk let waitress know want sit outside (since nice misters).  get two different servers bringing us stuff (talk service) ask one waitress recommendations.  didn't listen her, course, ordered southwestern burger w/ coleslaw started nice stack rings.\\\\n\\\\nthe onion rings perfectly cooked.  looked like prepackaged, crispy actually bite onion without pulling entire thing (don't hate that?!!!)\\\\n\\\\nthe southwestern burger order medium rare cooked accordingly.  soft, juicy, pink nice crispy browned outer layer can achieved well used grill.  creaminess chipotle mayo paired beautifully green chiles.  unfortunately, ate many onion rings, couldn't finish burger.  shame!\\\\n\\\\nthe coleslaw just like it.  hard find really good coleslaw.  prefer mine slightly sweet, not sour.  much vinegar slaw ruins opinion.  slaw perfect marriage mayo, vinegar, sugar. not mention carrots...\\\\n\\\\nmy experience great!  servers top notch kept water full entire time actually chatted us minutes.\\\\n\\\\nthere artist guy named ross every day 5393 days straight. no, not employee.  goes art! hasn't missed single day!!! like... 15 years! wanna seem 'in know' ask ross is... able tell you.\\\\n\\\\ntime nap!\", shape=(), dtype=string)\n",
      "Label-->  tf.Tensor(1, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# Consider a single example ----->\n",
    "example1, label1  = next(iter(train_ds))\n",
    "print(\"Example_text--> \", example1)\n",
    "print(\"Label--> \", label1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 249,
     "status": "ok",
     "timestamp": 1611465203291,
     "user": {
      "displayName": "Jyotisman Sahoo",
      "photoUrl": "",
      "userId": "12108646937534858560"
     },
     "user_tz": 360
    },
    "id": "9uf0bFWIcjL8",
    "outputId": "fdb4181f-cc7b-4224-a0d5-61bdd0b1737f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorized example text: tf.Tensor(\n",
      "[   1 3840 4951  170    1 3935  117  395  115 1659 7000 1712    1  128\n",
      "   38  176  443  249    1    1  115 2279  545 1311    2  715    1  298\n",
      "  217  200   40   58  372  270   86   28 6249    8   45  153  578 1647\n",
      "   18  422  609   12  176    9  200 2194   24 1776  887  336   27 6937\n",
      "  147  990 2790  323   28 3624    1  731 1498  613  305  175    6 6050\n",
      "  688  152  588  731  234 3695  468  100   19  744 7498 6937  147   30\n",
      "  740  943  305 5579  820 1208 1674   28  688 9745 6708 3184   21    1\n",
      "   33  204  978    1 1613 1704 3352 2805  486 7115  608  401  101  731\n",
      " 1498  231  779  147    1 2790    7    6], shape=(120,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# Now let's view its tokenized form\n",
    "print(\"vectorized example text:\",\n",
    "      tokenize_text(example1, label1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 248,
     "status": "ok",
     "timestamp": 1611465212836,
     "user": {
      "displayName": "Jyotisman Sahoo",
      "photoUrl": "",
      "userId": "12108646937534858560"
     },
     "user_tz": 360
    },
    "id": "xo6EH6VTc2Em",
    "outputId": "f1465a90-89fc-4d20-d742-e587424ea695"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 --> [UNK]\n",
      "2 --> p\n",
      "Vocabulary size --> 10000\n"
     ]
    }
   ],
   "source": [
    "# Checking a few of the indices to see if the mapping is as expected --->\n",
    "\n",
    "print(\"1 --> {}\".format(tokenize_layer.get_vocabulary()[1]))\n",
    "print(\"2 --> {}\".format(tokenize_layer.get_vocabulary()[3840]))\n",
    "print(\"Vocabulary size --> {}\".format(len(tokenize_layer.get_vocabulary())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can in principle convert all the text data into vectors as shown in the above example before feeding it into the model. This can speed up the training process but then one has to save the tokenizing indices separately for inference later as this is unique to the trained model. To generalize, we include the TextVectorization layer as part of the model building process. For anyone interested, the code to convert the data into indices before feeding to the model is also provided in the cell below but is commented out here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gH0IDUlleGC4"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "# Converting texts into token indices learned from the training files\n",
    "\n",
    "int_train_ds = train_ds.map(tokenize_text)\n",
    "int_valid_ds = valid_ds.map(tokenize_text)\n",
    "int_test_ds = test_ds.map(tokenize_text)\n",
    "\n",
    "\n",
    "for text_vec, _ in int_train_ds.take(2).as_numpy_iterator():\n",
    "  print(\"Text - {}\".format(text_vec))\n",
    "  \n",
    "\n",
    "# cache the prepared dataset in memory\n",
    "int_train_ds = int_train_ds.cache()\n",
    "int_valid_ds = int_valid_ds.cache()\n",
    "int_test_ds = int_test_ds.cache()\n",
    "\n",
    "# Preparing the dataset to be loaded into the model \n",
    "BUFFER_SIZE = 300000\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# for model without vectorization layer \n",
    "train_dataset = int_train_ds.shuffle(BUFFER_SIZE)\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "validation_dataset = int_valid_ds.batch(4).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = int_test_ds.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential([tf.keras.layers.Embedding(input_dim = VOCAB_SIZE, \n",
    "                                                       output_dim = 64, input_length = MAX_SEQUENCE_LENGTH),\n",
    "                             tf.keras.layers.Conv1D(128,5,activation='relu'),\n",
    "                             tf.keras.layers.GlobalAveragePooling1D(),\n",
    "                             tf.keras.layers.Dense(32, activation='relu'),\n",
    "                             tf.keras.layers.Dense(1,activation='sigmoid')])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cache the prepared dataset in memory\n",
    "train_ds = train_ds.cache()\n",
    "valid_ds = valid_ds.cache()\n",
    "test_ds = test_ds.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the dataset to be loaded into the model \n",
    "BUFFER_SIZE = 300000\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# for model with vectorization layer\n",
    "train_dataset = train_ds.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "validation_dataset = valid_ds.batch(4).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_ds.batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([tf.keras.Input(shape = (1,), dtype = tf.string),\n",
    "                             tokenize_layer,\n",
    "                             tf.keras.layers.Embedding(input_dim = VOCAB_SIZE, \n",
    "                                                       output_dim = 64, input_length = MAX_SEQUENCE_LENGTH),\n",
    "                             tf.keras.layers.Conv1D(128,5,activation='relu'),\n",
    "                             tf.keras.layers.GlobalAveragePooling1D(),\n",
    "                             tf.keras.layers.Dense(32, activation='relu'),\n",
    "                             tf.keras.layers.Dense(1,activation='sigmoid')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "text_vectorization (TextVect (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 120, 64)           640000    \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 116, 128)          41088     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 685,249\n",
      "Trainable params: 685,249\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "3gnW9afhIxuI"
   },
   "outputs": [],
   "source": [
    "model.compile(loss = 'binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 452095,
     "status": "ok",
     "timestamp": 1611466172299,
     "user": {
      "displayName": "Jyotisman Sahoo",
      "photoUrl": "",
      "userId": "12108646937534858560"
     },
     "user_tz": 360
    },
    "id": "KT7dIu_bI03r",
    "outputId": "9f02a1f8-46f8-4ff0-e7bb-237993a5c8bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "4375/4375 [==============================] - 210s 28ms/step - loss: 0.2500 - accuracy: 0.8887 - val_loss: 0.1663 - val_accuracy: 0.9354\n",
      "Epoch 2/5\n",
      "4375/4375 [==============================] - 68s 16ms/step - loss: 0.1529 - accuracy: 0.9397 - val_loss: 0.1563 - val_accuracy: 0.9393\n",
      "Epoch 3/5\n",
      "4375/4375 [==============================] - 69s 16ms/step - loss: 0.1304 - accuracy: 0.9491 - val_loss: 0.1523 - val_accuracy: 0.9385\n",
      "Epoch 4/5\n",
      "4375/4375 [==============================] - 69s 16ms/step - loss: 0.1120 - accuracy: 0.9569 - val_loss: 0.1646 - val_accuracy: 0.9369\n",
      "Epoch 5/5\n",
      "4375/4375 [==============================] - 70s 16ms/step - loss: 0.0950 - accuracy: 0.9639 - val_loss: 0.1657 - val_accuracy: 0.9363\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 5\n",
    "history = model.fit(train_dataset, epochs=NUM_EPOCHS, validation_data=validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "594/594 [==============================] - 6s 10ms/step - loss: 0.1559 - accuracy: 0.9408\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.15592601895332336, 0.9408420920372009]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_path = \"./my_models/{}\".format('sentiment_classifier_withVectorizer')\n",
    "\n",
    "model.save(saved_model_path, save_format = 'tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = tf.keras.models.load_model(\"./my_models/{}\".format('sentiment_classifier_withVectorizer'))\n",
    "\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 541
    },
    "executionInfo": {
     "elapsed": 615,
     "status": "ok",
     "timestamp": 1611466172934,
     "user": {
      "displayName": "Jyotisman Sahoo",
      "photoUrl": "",
      "userId": "12108646937534858560"
     },
     "user_tz": 360
    },
    "id": "eOUhOMUbIbYB",
    "outputId": "ae480d84-0e22-4cc5-fcd8-fcdfd1000224"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_graphs(history, string):\n",
    "  plt.plot(history.history[string])\n",
    "  plt.plot(history.history['val_'+string])\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(string)\n",
    "  plt.legend([string, 'val_'+string])\n",
    "  plt.show()\n",
    "\n",
    "plot_graphs(history, 'accuracy')\n",
    "plot_graphs(history, 'loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c2K56RnWTpJy"
   },
   "source": [
    "Even though the accuracies are high enough there is subastantial overfitting only after the first 2 epochs if the stopwords are not removed (one can check this by keeping the stopwords in the splits). After removing the stopwords (which is what we do here eventually), we still get overfitting but atleast towards the end of the training now. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2CLP1O0sIlTi"
   },
   "source": [
    "When we need to deploy the model, it is better to include as much of the preprocssing steps in the model directly. In the model earlier we did do part of it by making the TextVectorization layer part of the final model. We can further achieve this by including a customized callable into the 'standardize' parameter of the TextVectorization layer. This will significantly increase the training time but it is probably worth it later when we do inference. Let's take a look at it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SkhGCM2DLPfY"
   },
   "source": [
    "## TextVectorization with a custom standardize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "3dUT_dovTbuw"
   },
   "outputs": [],
   "source": [
    "# Function to be fed to the 'standardize' parameter. \n",
    "#Note the extra 'stopwords' removal step that was earlier outside the model  \n",
    "\n",
    "@tf.keras.utils.register_keras_serializable()\n",
    "def operations(text):\n",
    "  text = \" \" + text   # for removing stop words at the beginning of the text\n",
    "  text = tf.strings.lower(text) \n",
    "  stopwords = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]\n",
    "  for word in stopwords:\n",
    "    token = \" \" + word + \" \"\n",
    "    text = tf.strings.regex_replace(text, token, \" \")\n",
    "  remove_regex = f'[{re.escape(string.punctuation)}]'  \n",
    "  text = tf.strings.regex_replace(text, remove_regex, '') #removing punctuations\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "1wP8-GI1KRhP"
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 120\n",
    "VOCAB_SIZE = 10000\n",
    "\n",
    "tokenizer_layer = TextVectorization(\n",
    "  standardize=operations, # now with customized text preprocessing\n",
    "  max_tokens=VOCAB_SIZE,\n",
    "  output_mode='int',\n",
    "  output_sequence_length=MAX_SEQUENCE_LENGTH) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next steps are more or less the same as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "APyGlLkPLY0I"
   },
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "train_ds, info = tfds.load('yelp_polarity_reviews', split = 'train', as_supervised = True, with_info=True)\n",
    "valid_ds = tfds.load('yelp_polarity_reviews', split = 'test[:50%]', as_supervised = True)\n",
    "test_ds = tfds.load('yelp_polarity_reviews', split = 'test[50%:]', as_supervised = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "wSIlIuCYMXrA"
   },
   "outputs": [],
   "source": [
    "train_only_text = train_ds.map(lambda text, label : text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "TFLNXMFBLgrG"
   },
   "outputs": [],
   "source": [
    "tokenizer_layer.adapt(train_only_text.batch(2048))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 230,
     "status": "ok",
     "timestamp": 1611476836526,
     "user": {
      "displayName": "Jyotisman Sahoo",
      "photoUrl": "",
      "userId": "12108646937534858560"
     },
     "user_tz": 360
    },
    "id": "1FyU0bQoMn5d",
    "outputId": "a39d17aa-106f-4c3b-aa45-5977c1bbfc00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example_text-->  tf.Tensor(b\"The Groovy P. and I ventured to his old stomping grounds for lunch today.  The '5 and Diner' on 16th St and Colter left me with little to ask for.  Before coming here I had a preconceived notion that 5 & Diners were dirty and nasty. Not the case at all.\\\\n\\\\nWe walk in and let the waitress know we want to sit outside (since it's so nice and they had misters).  We get two different servers bringing us stuff (talk about service) and I ask the one waitress for recommendations.  I didn't listen to her, of course, and ordered the Southwestern Burger w/ coleslaw and started with a nice stack of rings.\\\\n\\\\nThe Onion Rings were perfectly cooked.  They looked like they were prepackaged, but they were very crispy and I could actually bite through the onion without pulling the entire thing out (don't you hate that?!!!)\\\\n\\\\nThe Southwestern Burger was order Medium Rare and was cooked accordingly.  Soft, juicy, and pink with a nice crispy browned outer layer that can only be achieved on a well used grill.  The creaminess of the chipotle mayo paired beautifully with the green chiles.  Unfortunately, because I ate too many onion rings, I couldn't finish my burger.  What a shame!\\\\n\\\\nThe Coleslaw was just how I like it.  It's hard to find a really good coleslaw.  I prefer mine to be slightly sweet, not sour.  Too much vinegar in slaw ruins it in my opinion.  This slaw had the perfect marriage of mayo, vinegar, and sugar. Not to mention carrots...\\\\n\\\\nMy experience here was great!  The servers were top notch and kept my water full the entire time and actually chatted with us for a few minutes.\\\\n\\\\nThere is an artist guy named Ross who has been there every day for 5393 days straight. No, not an employee.  He goes there and does his art! He hasn't missed a SINGLE day!!! That's like... 15 years! So if you wanna seem to be 'in the know' ask where Ross is... They'll be able to tell you.\\\\n\\\\nTime for a nap!\", shape=(), dtype=string)\n",
      "Label-->  tf.Tensor(1, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "example1, label1  = next(iter(train_ds))\n",
    "print(\"Example_text--> \", example1)\n",
    "print(\"Label--> \", label1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "QLtjFjkCODHg"
   },
   "outputs": [],
   "source": [
    "def tokenize_text2(text, label):\n",
    "  text = tf.expand_dims(text, -1)\n",
    "  return tokenizer_layer(text)[0], label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 295,
     "status": "ok",
     "timestamp": 1611473833122,
     "user": {
      "displayName": "Jyotisman Sahoo",
      "photoUrl": "",
      "userId": "12108646937534858560"
     },
     "user_tz": 360
    },
    "id": "LZCln7YZOur0",
    "outputId": "0e30a6d6-bbbf-4c3b-9c1f-de252014a975"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(120,), dtype=int64, numpy=\n",
      "array([   1, 3840, 4951,  170,    1, 3935,  117,  395,  115, 1659, 7000,\n",
      "       1712,    1,  128,   38,  176,  443,  249,    1,    1,  115, 2279,\n",
      "        545, 1311,    2,  715,    1,  298,  217,  200,   40,   58,  372,\n",
      "        270,   86,   28, 6249,    8,   45,  153,  578, 1647,   18,  422,\n",
      "        609,   12,  176,    9,  200, 2194,   24, 1776,  887,  336,   27,\n",
      "       6937,  147,  990, 2790,  323,   28, 3624,    1,  731, 1498,  613,\n",
      "        305,  175,    6, 6050,  688,  152,  588,  731,  234, 3695,  468,\n",
      "        100,   19,  744, 7498, 6937,  147,   30,  740,  943,  305, 5579,\n",
      "        820, 1208, 1674,   28,  688, 9745, 6708, 3184,   21,    1,   33,\n",
      "        204,  978,    1, 1613, 1704, 3352, 2805,  486, 7115,  608,  401,\n",
      "        101,  731, 1498,  231,  779,  147,    1, 2790,    7,    6])>, <tf.Tensor: shape=(), dtype=int64, numpy=1>)\n"
     ]
    }
   ],
   "source": [
    "print(tokenize_text2(example1,label1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 235,
     "status": "ok",
     "timestamp": 1611480245208,
     "user": {
      "displayName": "Jyotisman Sahoo",
      "photoUrl": "",
      "userId": "12108646937534858560"
     },
     "user_tz": 360
    },
    "id": "6z7U_Pi8ecis",
    "outputId": "6cb39e2f-b250-4c8c-9157-925791f27205"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ventured'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_layer.get_vocabulary()[4951]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "YA_bDe9xO6CP"
   },
   "outputs": [],
   "source": [
    "# Preparing the dataset to be loaded into the model \n",
    "BUFFER_SIZE = 300000\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataset = train_ds.shuffle(BUFFER_SIZE)\n",
    "train_dataset = train_ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "validation_dataset = valid_ds.batch(4).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_ds.batch(4).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "pF_E9yMVR-lp"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([tf.keras.Input(shape = (1,), dtype = tf.string),\n",
    "                             tokenizer_layer,\n",
    "                             tf.keras.layers.Embedding(input_dim = VOCAB_SIZE, \n",
    "                                                       output_dim = 64, input_length = MAX_SEQUENCE_LENGTH),\n",
    "                             tf.keras.layers.Conv1D(128,5,activation='relu'),\n",
    "                             tf.keras.layers.GlobalAveragePooling1D(),\n",
    "                             tf.keras.layers.Dense(32, activation='relu'),\n",
    "                             tf.keras.layers.Dense(1,activation='sigmoid')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "C1tyy2vqU4O-"
   },
   "outputs": [],
   "source": [
    "model.compile(loss = 'binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1019818,
     "status": "ok",
     "timestamp": 1611481299508,
     "user": {
      "displayName": "Jyotisman Sahoo",
      "photoUrl": "",
      "userId": "12108646937534858560"
     },
     "user_tz": 360
    },
    "id": "0aku1KLKU9cW",
    "outputId": "fb97f273-6cb9-4113-a3f3-907435d023dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "17500/17500 [==============================] - 235s 13ms/step - loss: 0.2225 - accuracy: 0.9057 - val_loss: 0.1592 - val_accuracy: 0.9381\n",
      "Epoch 2/5\n",
      "17500/17500 [==============================] - 234s 13ms/step - loss: 0.1470 - accuracy: 0.9419 - val_loss: 0.1544 - val_accuracy: 0.9382\n",
      "Epoch 3/5\n",
      "17500/17500 [==============================] - 234s 13ms/step - loss: 0.1227 - accuracy: 0.9527 - val_loss: 0.1709 - val_accuracy: 0.9349\n",
      "Epoch 4/5\n",
      "17500/17500 [==============================] - 234s 13ms/step - loss: 0.0969 - accuracy: 0.9646 - val_loss: 0.2067 - val_accuracy: 0.9317\n",
      "Epoch 5/5\n",
      "17500/17500 [==============================] - 234s 13ms/step - loss: 0.0752 - accuracy: 0.9738 - val_loss: 0.2513 - val_accuracy: 0.9266\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 5\n",
    "history = model.fit(train_dataset, epochs=NUM_EPOCHS, validation_data=validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6333,
     "status": "ok",
     "timestamp": 1611481305850,
     "user": {
      "displayName": "Jyotisman Sahoo",
      "photoUrl": "",
      "userId": "12108646937534858560"
     },
     "user_tz": 360
    },
    "id": "YdEOqUeWVBi6",
    "outputId": "3cb08a41-56c2-4210-d739-304aeb826f56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4750/4750 [==============================] - 8s 2ms/step - loss: 0.2327 - accuracy: 0.9293\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.23269431293010712, 0.929263174533844]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyM71Y1EhLcr+PUxL3SdhGsI",
   "collapsed_sections": [],
   "name": "sentiment_classifier.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
